{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from embedding_creator import preprocess\n",
    "\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             EMB_DIM ENC_HID_DIM DEC_HID_DIM ATTN_DIM\n",
      "                             EMB_DROPOUT NET_DROPOUT LR TEACHER SAVE_RESULTS\n",
      "                             EMB_SOURCE EMB_PORTINARI REMOVE_ACCENTS\n",
      "ipykernel_launcher.py: error: argument EMB_DIM: invalid int value: '/home/danielprado/.local/share/jupyter/runtime/kernel-a3c90507-87b2-420b-90bd-42414cf86aeb.json'\n"
     ]
    }
   ],
   "source": [
    "def boolean_string(s):\n",
    "    if s not in {'False', 'True'}:\n",
    "        raise ValueError('Not a valid boolean string')\n",
    "    return s == 'True'\n",
    "\n",
    "try: \n",
    "    parser = argparse.ArgumentParser(description='Arguments for VAER')\n",
    "\n",
    "    parser.add_argument('EMB_DIM', action=\"store\", type=int)\n",
    "    parser.add_argument('ENC_HID_DIM', action=\"store\", type=int)\n",
    "    parser.add_argument('DEC_HID_DIM', action=\"store\", type=int)\n",
    "    parser.add_argument('ATTN_DIM', action=\"store\", type=int)\n",
    "    parser.add_argument('EMB_DROPOUT', action=\"store\", type=float)\n",
    "    parser.add_argument('NET_DROPOUT', action=\"store\", type=float)\n",
    "    parser.add_argument('LR', action=\"store\", type=float)\n",
    "    parser.add_argument('TEACHER', action=\"store\", type=float)\n",
    "    parser.add_argument('SAVE_RESULTS', action=\"store\")\n",
    "    \n",
    "    parser.add_argument('EMB_SOURCE', default= 'glove.6B.300d.NILC.txt', action=\"store\")\n",
    "    parser.add_argument('EMB_PORTINARI',\n",
    "                        default='../data/data-portinari/embedding_matrix_NILC.npy', action=\"store\")\n",
    "    parser.add_argument('REMOVE_ACCENTS', default=False, action=\"store\", type=boolean_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(parser.parse_args())\n",
    "\n",
    "    read = parser.parse_args()\n",
    "    \n",
    "    EMB_DIM = read.EMB_DIM\n",
    "    ENC_HID_DIM = read.ENC_HID_DIM\n",
    "    DEC_HID_DIM = read.DEC_HID_DIM\n",
    "    ATTN_DIM = read.ATTN_DIM\n",
    "    EMB_DROPOUT = read.EMB_DROPOUT\n",
    "    NET_DROPOUT = read.NET_DROPOUT\n",
    "    LR = read.LR\n",
    "    TEACHER = read.TEACHER\n",
    "    SAVE_RESULTS = read.SAVE_RESULTS\n",
    "    EMB_SOURCE = read.EMB_SOURCE\n",
    "    EMB_PORTINARI = read.EMB_PORTINARI\n",
    "    REMOVE_ACCENTS = read.REMOVE_ACCENTS\n",
    "    \n",
    "    print(EMB_SOURCE, EMB_PORTINARI, REMOVE_ACCENTS)\n",
    "    print(read)\n",
    "    '''EMB_SOURCE = read.EMB_SOURCE\n",
    "    EMB_PORTINARI = read.EMB_PORTINARI'''\n",
    "    \n",
    "except:\n",
    "    \n",
    "    #---------------ARGS-----------------#\n",
    "    EMB_DIM = 128\n",
    "    ENC_HID_DIM = 256\n",
    "    DEC_HID_DIM = 256\n",
    "    ATTN_DIM = 8\n",
    "    EMB_DROPOUT = 0.5\n",
    "    NET_DROPOUT = 0.5\n",
    "    LR = 0.004\n",
    "    TEACHER = 0.5\n",
    "    SAVE_RESULTS = '../results/results-portinari/progress_text_experiment_00' \n",
    "    EMB_SOURCE = 'glove.6B.300d.NILC.txt'\n",
    "    EMB_PORTINARI = '../data/data-portinari/embedding_matrix_NILC.npy'\n",
    "    REMOVE_ACCENTS = False\n",
    "\n",
    "ENC_EMB_DIM = EMB_DIM\n",
    "DEC_EMB_DIM = EMB_DIM\n",
    "ENC_HID_DIM = ENC_HID_DIM\n",
    "DEC_HID_DIM = DEC_HID_DIM\n",
    "ATTN_DIM = ATTN_DIM\n",
    "ENC_DROPOUT = EMB_DROPOUT\n",
    "DEC_DROPOUT = EMB_DROPOUT\n",
    "NET_DROPOUT = NET_DROPOUT\n",
    "TEACHER = TEACHER\n",
    "LR = LR\n",
    "SAVE_RESULTS = SAVE_RESULTS\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "LOG_INTERVAL = 40 \n",
    "MAX_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achou 526 retratos\n",
      "Indexing word vectors.\n",
      "Found 929607 word vectors.\n",
      "Null word embeddings: 1\n",
      "total words 538345\n",
      "null words 2199\n",
      "0.41 % de palavras não encontradas no emb\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, portinari_idx, retratos_idx, resto_idx, word_index, index_word = preprocess(True, \n",
    "        embedding_source = EMB_SOURCE, embedding_portinari = EMB_PORTINARI, remove_accents = REMOVE_ACCENTS)\n",
    "\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class PortinariDesc(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "#cria o dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_retratos, val_retratos = model_selection.train_test_split(retratos_idx, test_size = 0.2, shuffle = True)\n",
    "train_resto, val_resto = model_selection.train_test_split(resto_idx, test_size = 0.2, shuffle = True)\n",
    "\n",
    "\n",
    "train = train_retratos + train_resto\n",
    "val = val_retratos + val_resto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_tensor = list(map(lambda x: torch.tensor(x), train))\n",
    "val_tensor = list(map(lambda x: torch.tensor(x), val))\n",
    "\n",
    "train_dataset = PortinariDesc(train_tensor)\n",
    "val_dataset = PortinariDesc(val_tensor)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    xx = batch\n",
    "    x_lens = list(map(len, xx))\n",
    "\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0, )\n",
    "\n",
    "    return xx_pad, x_lens\n",
    "\n",
    "#embedding_torch = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palavras desconhecidas: 294\n"
     ]
    }
   ],
   "source": [
    "unknown_tokens = [i for i in range(len(embedding_matrix)) if np.sum(embedding_matrix[i]-embedding_matrix[-3]) == 0]\n",
    "for tk in unknown_tokens:\n",
    "    index_word[tk] = '<unk>'\n",
    "for w, idx in word_index.items():\n",
    "    if np.sum(embedding_matrix[idx])==0:\n",
    "        word_index[w] = len(embedding_matrix)-1\n",
    "palavras_desconhecidas = [w for w, idx in word_index.items()  if np.sum(embedding_matrix[idx]-embedding_matrix[-3]) == 0]\n",
    "print(f'palavras desconhecidas: {len(palavras_desconhecidas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3',\n",
       " '4',\n",
       " 'pixains',\n",
       " 'esvoaçados',\n",
       " 'papilha',\n",
       " 'batéia',\n",
       " 'payots',\n",
       " 'mancho',\n",
       " '2',\n",
       " 'batatudo',\n",
       " '1',\n",
       " 'grisados',\n",
       " 'semilongos',\n",
       " 'hachuriados',\n",
       " 'jabô',\n",
       " '¡reas',\n",
       " 'antùnio',\n",
       " 'composiá\\x84o',\n",
       " 'geometrizando',\n",
       " 'hubrecht',\n",
       " '·rea',\n",
       " 'itaguahy',\n",
       " 'enfumadas',\n",
       " '5',\n",
       " 'mouries',\n",
       " 'endomingada',\n",
       " 'pregamento',\n",
       " 'maáaroca',\n",
       " 'simétricamente',\n",
       " 'esvoaçada',\n",
       " 'esfumato',\n",
       " 'isûsceles',\n",
       " 'semicerrado',\n",
       " 'antebraáo',\n",
       " 'aquareladas',\n",
       " 'zínea',\n",
       " 'ranhaduras',\n",
       " 'est·',\n",
       " '¡rea',\n",
       " 'aquarelado',\n",
       " 'kipás',\n",
       " '8',\n",
       " 'retratato',\n",
       " 'sanfonado',\n",
       " 'chapèu',\n",
       " 'falangeta',\n",
       " 'tiberiade',\n",
       " 'verùnica',\n",
       " 'abatatado',\n",
       " 'esfumaturas',\n",
       " 'entintamento',\n",
       " 'planoa',\n",
       " '21',\n",
       " 'craquelê',\n",
       " 'heptagon',\n",
       " '90',\n",
       " 'ampuleta',\n",
       " 'gabirobas',\n",
       " 'pùr',\n",
       " 'sgraffito',\n",
       " 'entrefechado',\n",
       " 'zíneas',\n",
       " 'm\\x84os',\n",
       " 'extens\\x84o',\n",
       " 'tínues',\n",
       " '·rvore',\n",
       " 'constratando',\n",
       " 'balangandans',\n",
       " '1941',\n",
       " 'boiserie',\n",
       " 'beduino',\n",
       " 'beduina',\n",
       " 'craquelado',\n",
       " 'hachuriado',\n",
       " 'emaranhas',\n",
       " 'geometrismos',\n",
       " 'gontrau',\n",
       " 'esvoaçadas',\n",
       " 'carbrito',\n",
       " 'retratatado',\n",
       " 'esfumatura',\n",
       " 'boisérie',\n",
       " 'darclée',\n",
       " 'conseq¸íncia',\n",
       " 'disposiá\\x84o',\n",
       " 'capitonné',\n",
       " 'falanginha',\n",
       " 'jesrael',\n",
       " 'direá\\x84o',\n",
       " 'evangelhista',\n",
       " 'escarioto',\n",
       " 'gnatali',\n",
       " 'píra',\n",
       " 'geometrizam',\n",
       " 'cravá',\n",
       " 'hipópotamo',\n",
       " 'suregem',\n",
       " 'cacheadas',\n",
       " 'badeixo',\n",
       " 'willemsens',\n",
       " 'caìram',\n",
       " 'esboáados',\n",
       " 'enchapelado',\n",
       " '¡vila',\n",
       " 'esqu·lido',\n",
       " 'lìder',\n",
       " 'tarquìnio',\n",
       " 'afraninho',\n",
       " '20',\n",
       " 'sedosidade',\n",
       " 'tedrás',\n",
       " 'soliéu',\n",
       " 'cuvado',\n",
       " 'isóceles',\n",
       " 'bananeiral',\n",
       " 'sugetão',\n",
       " 'llambi',\n",
       " 'telhadoçom',\n",
       " 'polland',\n",
       " 'esboc',\n",
       " 'despregamentos',\n",
       " 'composic',\n",
       " 'camposição',\n",
       " 'tambe',\n",
       " 'segerindo',\n",
       " 'repesentando',\n",
       " 'espéice',\n",
       " 'encardenação',\n",
       " 'geométicas',\n",
       " 'retângulares',\n",
       " 'groseiros',\n",
       " 'minusiosa',\n",
       " 'semilevantado',\n",
       " 'contranstando',\n",
       " 'urubús',\n",
       " 'iramãos',\n",
       " 'craquele',\n",
       " 'goemetrizada',\n",
       " 'rerpesentada',\n",
       " 'campanheiro',\n",
       " 'representano',\n",
       " 'definindos',\n",
       " 'preminentes',\n",
       " 'tóraxes',\n",
       " 'rerpesentando',\n",
       " 'difinindo',\n",
       " 'entolada',\n",
       " 'lver',\n",
       " 'caraveka',\n",
       " 'cùncava',\n",
       " 'semirasteira',\n",
       " 'õndia',\n",
       " 'volitas',\n",
       " 'arrebitando',\n",
       " 'bandeau',\n",
       " 'fisionùmicos',\n",
       " 'proeminíncia',\n",
       " 'pictûrica',\n",
       " 'interpenetrando',\n",
       " 'identific·veis',\n",
       " 'aldary',\n",
       " '6',\n",
       " '7',\n",
       " 'p·ssaros',\n",
       " 'cûs',\n",
       " 'amarados',\n",
       " 'bandeijinha',\n",
       " 'g·spea',\n",
       " '1x1cm',\n",
       " '3x3cm',\n",
       " 'equínio',\n",
       " 'festoné',\n",
       " 'cìrculos',\n",
       " 'exceá\\x84o',\n",
       " 'entrecerrada',\n",
       " 'dègradè',\n",
       " 'marac·',\n",
       " 'píras',\n",
       " 'pès',\n",
       " 'tûrax',\n",
       " 'indìgena',\n",
       " 'chaminè',\n",
       " 'josetti',\n",
       " 'semicìrculo',\n",
       " 'oblìquos',\n",
       " 'jabôs',\n",
       " 'hept·gono',\n",
       " 'antebraáos',\n",
       " 'embicada',\n",
       " 'esqu·lidas',\n",
       " 'jabots',\n",
       " 'ombrearas',\n",
       " '\\x82ngulos',\n",
       " 'bassé',\n",
       " 'concíntricos',\n",
       " 'aferes',\n",
       " 'margaridinhas',\n",
       " 'transparíncia',\n",
       " 'pergentina',\n",
       " '¡·rea',\n",
       " 'hahneman',\n",
       " 'espatulamento',\n",
       " 'cûrrego',\n",
       " 'entardecendo',\n",
       " 'sang¸ìnea',\n",
       " 'p·lpebra',\n",
       " 'tri\\x82ngulo',\n",
       " 'sianinha',\n",
       " 'neuding',\n",
       " 'semilonga',\n",
       " 'tr·s',\n",
       " 'amendoadas',\n",
       " 'cucè',\n",
       " '4x4cm',\n",
       " 'express\\x84o',\n",
       " 'escatilhada',\n",
       " '90º',\n",
       " '6ª',\n",
       " '1792',\n",
       " 'constrûem',\n",
       " 'chimarão',\n",
       " 'balè',\n",
       " 'heitgen',\n",
       " 'ret\\x82ngulos',\n",
       " 'amarrilhos',\n",
       " 'enfumada',\n",
       " 'orberg',\n",
       " 'esponjada',\n",
       " 'losângulos',\n",
       " 'fincá',\n",
       " 'peneirasse',\n",
       " 'destramada',\n",
       " 'micagem',\n",
       " 'parnaltas',\n",
       " '2x2cm',\n",
       " 'morçura',\n",
       " 'arámburu',\n",
       " 'palaninho',\n",
       " 'vêndo',\n",
       " 'siméricamente',\n",
       " 'decúbido',\n",
       " 'bateeiros',\n",
       " 'texturizando',\n",
       " 'têz',\n",
       " 'ensacadores',\n",
       " 'sermolino',\n",
       " 'adastadas',\n",
       " 'carnaúbeiras',\n",
       " 'hachuriada',\n",
       " 'tronquinhos',\n",
       " 'craquelês',\n",
       " 'salpigados',\n",
       " 'esquemáticamente',\n",
       " 'horinzontal',\n",
       " 'garcéz',\n",
       " 'zavalia',\n",
       " 'abotuadura',\n",
       " 'arranhacéus',\n",
       " 'representaçãode',\n",
       " 'rolotí',\n",
       " 'sû',\n",
       " 'elrick',\n",
       " 'aroch',\n",
       " 'amcotts',\n",
       " 'désy',\n",
       " 'direçãodo',\n",
       " 'adernada',\n",
       " 'geomètricas',\n",
       " 'trís',\n",
       " 'paletû',\n",
       " 'sobressaìrem',\n",
       " 'sobrepiem',\n",
       " 'roteira',\n",
       " 'winitsky',\n",
       " 'ligeirametne',\n",
       " 'lóló',\n",
       " 'destinguem',\n",
       " 'johtm',\n",
       " 'molofote',\n",
       " 'gregolini',\n",
       " 'tassano',\n",
       " 'rabech',\n",
       " 'centando',\n",
       " 'descaláas',\n",
       " 'theódolo',\n",
       " '5032',\n",
       " 'theotoky',\n",
       " 'representanção',\n",
       " 'espaldadas',\n",
       " 'geometricas',\n",
       " 'esfumatos',\n",
       " '18',\n",
       " '16x4',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras_desconhecidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#palavras_desconhecidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.distributions.categorical.Categorical(torch.Tensor([[3, 0.0, 3], [0.3, 0.4, 0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tensor = torch.Tensor(embedding_matrix).to(device)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, drop_last=True,\n",
    "    batch_size = BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, drop_last=True,\n",
    "    batch_size = BATCH_SIZE, shuffle=True, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from text_models import Encoder, Decoder, Attention, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from skimage.io import imread\n",
    "from torchsummary import summary\n",
    "import pandas as pd\n",
    "import time\n",
    "from text_models import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pre_trained_embedding: float,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=pre_trained_embedding)\n",
    "        self.emb_dim = pre_trained_embedding.shape[1]\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "            \n",
    "        self.rnn = nn.GRU(self.emb_dim, enc_hid_dim, bidirectional = True)\n",
    "\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                enc_input: Tensor, \n",
    "                enc_input_len) -> Tuple[Tensor]:\n",
    "\n",
    "        embedded = self.dropout(self.embedding(enc_input))\n",
    "        \n",
    "        embedded_pack = torch.nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "            enc_input_len, enforce_sorted=False)\n",
    "        \n",
    "        #print(embedded_pack)\n",
    "        outputs_packed, hidden = self.rnn(embedded_pack)\n",
    "        #print(outputs_packed[0].shape)\n",
    "        #print(outputs_packed[1].shape)\n",
    "        #print(outputs_packed.shape)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs_packed)\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        #print(hidden.shape)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "\n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "\n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "\n",
    "    def forward(self,\n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden,\n",
    "            encoder_outputs),\n",
    "            dim = 2)))\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pre_trained_embedding: float,\n",
    "                 enc_hid_dim: int,\n",
    "                 dec_hid_dim: int,\n",
    "                 dropout: int,\n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embeddings=pre_trained_embedding)\n",
    "        self.output_dim = pre_trained_embedding.shape[0]\n",
    "        self.emb_dim = pre_trained_embedding.shape[1]\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + self.emb_dim, dec_hid_dim)\n",
    "\n",
    "        self.out = nn.Linear(self.attention.attn_in + self.emb_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def _weighted_encoder_rep(self,\n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "        #print('a shape = {}'.format(a.shape))\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #print('encoder_outputs shape = {}'.format(encoder_outputs.shape))\n",
    "        \n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "        #print('weighted_encoder_rep shape = {}'.format(weighted_encoder_rep.shape))\n",
    "        \n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "        #print('weighted_encoder_rep shape = {}'.format(weighted_encoder_rep.shape))\n",
    "        \n",
    "        return weighted_encoder_rep\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                dec_input: Tensor,\n",
    "                #dec_input_len: Tensor, \n",
    "                decoder_hidden: Tensor,\n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "\n",
    "        dec_input = dec_input.unsqueeze(0)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(dec_input))\n",
    "        #print(embedded.shape)\n",
    "        #print(f'embedded shape = {embedded.shape}')\n",
    "        weighted_h = self._weighted_encoder_rep(decoder_hidden,\n",
    "                                                encoder_outputs)\n",
    "        \n",
    "        #print(f'weighted_h shape = {weighted_h.shape}')\n",
    "        rnn_input = torch.cat((embedded, weighted_h), dim = 2)\n",
    "        \n",
    "        #print(rnn_input.shape)\n",
    "        #print(decoder_hidden.unsqueeze(0).shape)\n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_h = weighted_h.squeeze(0)\n",
    "\n",
    "        output = self.out(torch.cat((output,\n",
    "                                     weighted_h,\n",
    "                                     embedded), dim = 1))\n",
    "\n",
    "        return output, decoder_hidden.squeeze(0)    \n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                x: Tensor,\n",
    "                x_l : Tensor,\n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "\n",
    "        batch_size = x.shape[1]\n",
    "        max_len = x.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(max_len-1, batch_size, vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(x, x_l)\n",
    "\n",
    "        # first input to the decoder is the <sos> token\n",
    "        output = x[0,:]\n",
    "        #print(f'first output shape = {output}')\n",
    "        \n",
    "        for t in range(0, max_len-1):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            #torch.distributions.categorical.Categorical(torch.Tensor([0.1, 0.0, 0.8]))\n",
    "            #print(output.shape)\n",
    "            #categorical_output = torch.distributions.categorical.Categorical(output)\n",
    "            #top1 = categorical_output.sample()\n",
    "            #print(top1)\n",
    "            #print(top1.shape)\n",
    "            output = (x[t+1,:] if teacher_force else top1)\n",
    "            #if teacher_force: \n",
    "            #    print(f'output teacher force = {output}')\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ATTN_DIM = 64\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5'''\n",
    "\n",
    "\n",
    "\n",
    "enc = Encoder(embedding_tensor, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT).to(device)\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM).to(device)\n",
    "dec = Decoder(embedding_tensor, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT,  attn).to(device)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,909,442 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import math\\nimport time\\n\\n(x,x_l) = next(iter(train_data_loader))\\nprint(x)\\nx = x.to(device)\\nx_l = torch.Tensor(x_l).to(device)\\noutput = model(x.permute(1,0),x_l)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import math\n",
    "import time\n",
    "\n",
    "(x,x_l) = next(iter(train_data_loader))\n",
    "print(x)\n",
    "x = x.to(device)\n",
    "x_l = torch.Tensor(x_l).to(device)\n",
    "output = model(x.permute(1,0),x_l)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          iterator: DataLoader,\n",
    "          optimizer: optim.Optimizer,\n",
    "          criterion: nn.Module,\n",
    "          clip: float,\n",
    "          teacher : float,\n",
    "          epoch):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_start = 0\n",
    "    train_loss = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    for batch_idx, (x,x_l) in enumerate(iterator):\n",
    "        \n",
    "        x = x.to(device)\n",
    "       \n",
    "        x_l = torch.Tensor(x_l).to(device)\n",
    "        \n",
    "        #print(f\"x = {x.shape}, x_l = {x_l.shape}\")\n",
    "        \n",
    "        output = model(x.permute(1,0),x_l, teacher)\n",
    "        #output = output[:,1:,:]\n",
    "        flat_output = output.view(-1, output.shape[-1])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        \n",
    "        \n",
    "        target = x[:,1:].permute(1,0).contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(flat_output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        \n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "                interval = time.time() - start\n",
    "                start = time.time()\n",
    "                epoch_start = epoch_start + interval\n",
    "\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss : {:.6f} \\tTime Interv: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(x), len(iterator.dataset),\n",
    "                           100. * batch_idx / len(iterator),\n",
    "                           loss.item(), interval))\n",
    "        #del(x)\n",
    "        #del(x_l)\n",
    "        #torch.cuda.empty_cache()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module,\n",
    "             iterator: DataLoader,\n",
    "             criterion: nn.Module):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, (x,x_l) in enumerate(iterator):\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            x_l = torch.Tensor(x_l).to(device)\n",
    "            \n",
    "            output = model(x.permute(1,0),x_l, 0)\n",
    "            #output = output[:,1:,:]\n",
    "            \n",
    "            flat_output = output.view(-1, output.shape[-1])\n",
    "            target = x[:,1:].permute(1,0).contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(flat_output, target)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            del(x)\n",
    "            del(x_l)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/3630 (0%)]\tLoss : 9.046227 \tTime Interv: 2.731362\n",
      "Train Epoch: 0 [640/3630 (18%)]\tLoss : 6.051653 \tTime Interv: 41.074471\n",
      "Train Epoch: 0 [1280/3630 (35%)]\tLoss : 5.897942 \tTime Interv: 41.785329\n",
      "Train Epoch: 0 [1920/3630 (53%)]\tLoss : 5.897379 \tTime Interv: 41.002971\n",
      "Train Epoch: 0 [2560/3630 (71%)]\tLoss : 5.726577 \tTime Interv: 40.924388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a8b154b0bb7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEACHER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5ac5640dc0e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, teacher, epoch)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch-danielprado/miniconda3/envs/portinari_pytorch_1.3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scratch-danielprado/miniconda3/envs/portinari_pytorch_1.3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def epoch_time(start_time: int,\n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "last_valid_loss = 100\n",
    "valid_loss_patience = 100\n",
    "valid_loss_register = []\n",
    "early_stop_patience = 5\n",
    "early_stop_register = []\n",
    "\n",
    "\n",
    "#for epoch in range(N_EPOCHS):\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_data_loader, optimizer, criterion, CLIP, TEACHER, epoch)\n",
    "    valid_loss = evaluate(model, val_data_loader, criterion)\n",
    "    \n",
    "    early_stop_register.append(valid_loss-last_valid_loss)\n",
    "    valid_loss_register.append(valid_loss)\n",
    "    \n",
    "    \n",
    "    if epoch >= early_stop_patience + 3:\n",
    "        if valid_loss_register[-3] + sum(early_stop_register[-2:]) > 1*valid_loss_register[-3]:\n",
    "            print(f'ref loss at -3  {valid_loss_register[-3]}')\n",
    "            print(f'last 2 variations: {early_stop_register[-2:]}')\n",
    "            break\n",
    "            \n",
    "    if epoch >= early_stop_patience + 5:\n",
    "        if valid_loss_register[-5] + sum(early_stop_register[-4:]) > 0.95*valid_loss_register[-5]:\n",
    "            print(f'ref loss at -5 {valid_loss_register[-5]}')\n",
    "            print(f'last 4 variations: {early_stop_register[-4:]}')\n",
    "            break\n",
    "            \n",
    "\n",
    "    last_valid_loss = valid_loss\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "try: \n",
    "    progress = np.load(SAVE_RESULTS + \".ndpy\")\n",
    "    progress = progress.tolist()\n",
    "    progress.append((str(read), train_loss, valid_loss))\n",
    "    np.save(SAVE_RESULTS, progress) \n",
    "except FileNotFoundError: \n",
    "    np.save(SAVE_RESULTS, []) \n",
    "except:\n",
    "    print('unkown exception!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def print_sents():\n",
    "with torch.no_grad():\n",
    "    model.train()\n",
    "    (x,x_l) = next(iter(train_data_loader))\n",
    "    x = x.to(device)\n",
    "    x_l_tens = torch.Tensor(x_l).to(device)\n",
    "    output = model(x.permute(1,0),x_l_tens, 0)\n",
    "    out = output.argmax(-1).transpose(1,0).cpu().numpy().tolist()\n",
    "    inp = x.cpu().numpy().tolist()\n",
    "    inp_text = [[index_word[token] if token != 0 else 'NUll' for token in sentence ] \n",
    "              for sentence in inp]\n",
    "    out_text = [[index_word[token] if token != 0 else 'NUll' for token in sentence ] \n",
    "                  for sentence in out]\n",
    "\n",
    "    print('\\nTrain = True')\n",
    "    print('\\nfrase inp:', inp_text[0][0:x_l[0]+1])\n",
    "    print('\\nfrase out:',  out_text[0][0:x_l[0]+1])\n",
    "\n",
    "    print('\\nTrain = False')\n",
    "\n",
    "    model.eval()\n",
    "    output = model(x.permute(1,0),x_l_tens, 0)\n",
    "    out = output.argmax(-1).transpose(1,0).cpu().numpy().tolist()\n",
    "    inp = x.cpu().numpy().tolist()\n",
    "    inp_text = [[index_word[token] if token != 0 else 'NUll' for token in sentence ] \n",
    "              for sentence in inp]\n",
    "    out_text = [[index_word[token] if token != 0 else 'NUll' for token in sentence ] \n",
    "                  for sentence in out]\n",
    "    print('\\nfrase inp:', inp_text[0][0:x_l[0]+1])\n",
    "    print('\\nfrase out:',  out_text[0][0:x_l[0]+1])\n",
    "\n",
    "#print_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x[:,1:].permute(1,0).contiguous().view(-1).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.view(-1, output.shape[-1]).max(1)[0].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
